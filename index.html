<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.6"/>
<title>MiRo-Training: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/search.js"></script>
<script type="text/javascript">
  $(document).ready(function() { searchBox.OnSelectItem(0); });
</script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td style="padding-left: 0.5em;">
   <div id="projectname">MiRo-Training
   </div>
   <div id="projectbrief">The aim of the project is to develop a software architecture for interacting with Miro using vocal and gestural commands.</div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.6 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
  <div id="navrow1" class="tabs">
    <ul class="tablist">
      <li class="current"><a href="index.html"><span>Main&#160;Page</span></a></li>
      <li><a href="annotated.html"><span>Classes</span></a></li>
      <li><a href="files.html"><span>Files</span></a></li>
      <li>
        <div id="MSearchBox" class="MSearchBoxInactive">
        <span class="left">
          <img id="MSearchSelect" src="search/mag_sel.png"
               onmouseover="return searchBox.OnSearchSelectShow()"
               onmouseout="return searchBox.OnSearchSelectHide()"
               alt=""/>
          <input type="text" id="MSearchField" value="Search" accesskey="S"
               onfocus="searchBox.OnSearchFieldFocus(true)" 
               onblur="searchBox.OnSearchFieldFocus(false)" 
               onkeyup="searchBox.OnSearchFieldChange(event)"/>
          </span><span class="right">
            <a id="MSearchClose" href="javascript:searchBox.CloseResultsWindow()"><img id="MSearchCloseImg" border="0" src="search/close.png" alt=""/></a>
          </span>
        </div>
      </li>
    </ul>
  </div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
<a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(0)"><span class="SelectionMark">&#160;</span>All</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(1)"><span class="SelectionMark">&#160;</span>Classes</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(2)"><span class="SelectionMark">&#160;</span>Files</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(3)"><span class="SelectionMark">&#160;</span>Functions</a><a class="SelectItem" href="javascript:void(0)" onclick="searchBox.OnSelectItem(4)"><span class="SelectionMark">&#160;</span>Variables</a></div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">MiRo-Training Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h2>The Project</h2>
<p>This Project has been developed for the Social Robotics course of the master degree program in Robotics Engineering at University of Genoa.</p>
<h3>MiRo Companion Robot</h3>
<p>MiRo is a animal-like robot developed as a prototype companion. It was designed with a bio-inspired architecture based on the neuroscience knowledge of the mammalian brain.</p>
<h3>The Objective</h3>
<p>The aim of the project is to develop a software architecture for interacting with Miro using vocal and gestural commands. The robot attention is obtained through the vocal activation command **"Miro"**. Only after the activation command the robot is able to execute further commands. The possibilities are the following:</p>
<ul>
<li>**"Good"** - The robot expresses a cheerful and noisy behaviour and requires to be touched by the user to calm down.</li>
<li>**"Bad"** - The robot becomes upsed for being scolded and turn its back to the user.</li>
<li>**"Let's go out"** - The robot leaves the charge to the user that can control its body movement with gestures. The gesture to use are summarized <a href="https://ibb.co/6nLxgjw">here</a></li>
<li>**"Play"** - The robot "follows" the movement of a red ball.</li>
<li>**"Kill"** - The robot becomes angry. It lights up in red and produces pirate sounds.</li>
<li>**"Sleep"** - The robot goes in a resting mode. It disables the activation command. Hence, it is not able anymore to execute the other commands until a new command "Miro" wakes it up.</li>
</ul>
<h2><a href="https://ibb.co/4MggTyw">The Software Architecture</a></h2>
<p>The Software architecture can be seen by clicking on the section title. It shows the ROS node (blocks) and the rostopic (blue) used by the nodes to communicate.</p>
<p>Each module part of the architecture has been implemented as a ROS node. To make communication between the nodes possible, a Publish/Subscribe messaging pattern has been used. The Architecture has been organized to be modular, scalable and reusable in few steps. The characteristics of each node will be analysed more deeply in the following. </p>
<h3>Description of the Modules</h3>
<ul>
<li><b><a class="el" href="command__recognition_8py.html" title="The node command_recognition.py recognize vocal command converted as String and publish the associate...">command_recognition.py</a></b>: this node handles the vocal commands and decides which of them MiRo has to be execute, by publishing on the */miro/rob01/platform/control* topic. In order to be reactive to the instructions, MiRo has to be awakened through the vocal command "Miro".</li>
<li><b><a class="el" href="gbb__miro_8py.html" title="The node gbb_miro.py subscribes to the linear and angular velocity mapped in the imu_data_map node an...">gbb_miro.py</a></b>: this node is execute with the vocal command "Let's go out" and handles the gesture based behaviour of Miro. The smartwatch publishes the IMU data on the ** topic. The <b><a class="el" href="imu__data__map_8py.html" title="The node imu_data_map.py subscribes to the smartwatch&#39;s accelerometer data and publish the linear and...">imu_data_map.py</a></b> node reads the accelerations' values from the */inertial* topic and maps them into linear and angular velocities, publishing the resulting data on the */imu_mapping* topic. The <b><a class="el" href="gbb__miro_8py.html" title="The node gbb_miro.py subscribes to the linear and angular velocity mapped in the imu_data_map node an...">gbb_miro.py</a></b> node implements the gesture based behaviour in which the robot follows the user's gestures. It publishes the linear and angular velocities on the */gbb* topic.</li>
<li><b><a class="el" href="play_8py.html" title="The node play.py implements the action corresponding to the command &quot;Play&quot;. ">play.py</a></b>: this node is executed with the vocal command "Play" and publishes on the ** topic. MiRo responds by following the red ball that the user shows him. The right and left camera of MiRo are used for the ball detection. The <b>hsv_color_filter</b> node performs the color segmentation of the image obtained from the cameras and publishes on the */right-left/hsv_color_filter/image*. The segmented image is acquired by the <b>hough_circles</b> that detects round object, highlighting it with a red circle. If the red ball is detected in the right/left camera MiRo starts turning right/left, until the object is contained in both the cameras. At this point the robot starts moving towards the ball and when it reaches it, it stops.</li>
<li><b><a class="el" href="bad_8py.html" title="The node bad.py implements the action corresponding to the command &quot;Bad&quot;. ">bad.py</a></b>: this node is executed with the vocal command "Bad" and publishes on the */miro_bad* topic. MiRo responds to the reproach with a sad and offended behaviour. It lowers and inclines the head, rotates the ears and rotates to its left, showing the back to the user. At the end of the action the leds light up in red, expressing its upset state.</li>
<li><b><a class="el" href="kill_8py.html" title="The node kill.py implements the action corresponding to the command &quot;Kill&quot;. ">kill.py</a></b>: this node is executed with the vocal command "Kill" and publishes on the **topic. MiRo responds with a angry behavior where it lights up in red and emitts pirate sounds.</li>
<li><b><a class="el" href="good_8py.html" title="The node good.py implements the action corresponding to the command &quot;Good&quot;. ">good.py</a></b>: this node is executed with the vocal command "Good" and publishes on the ** topic. MiRo tries to capture the user's attention by raising up his head and wagging the tail. In this mode the user can touch MiRo on the head or on the body, generating two different behaviours that depend on the values of the sensors published by the */miro_rob01_platform_sensors* topic. When the sensors of the head are activated the robot stops the tail, lowers and inclines the head, squint the eyes and the leds light up in pink. When the sensors on the body are activated the robot stops the tail, raises up the head, rotates the ears and the leds light up in orange.</li>
<li><b><a class="el" href="sleep_8py.html" title="The node sleep.py implements the action corresponding to the command &quot;Sleep&quot;   Miro closes its eyes...">sleep.py</a></b>: this node is executed with the vocal command "Sleep" and publishes on the ** topic. MiRo responds by entering in a rest mode. It lowers and inclines its head and tail, closes the eyes and the leds light up in aquamarine. In this mode the other vocal commands have no effect, unless it is activated with a new vocal command "MiRo"</li>
</ul>
<h1>Gettin Started</h1>
<h2>Prerequisites</h2>
<h3>ROS</h3>
<p>This project is developed using <a href="http://wiki.ros.org/kinetic/Installation/Ubuntu">ROS</a>:</p>
<ul>
<li>rosdistro: kinetic</li>
<li>rosversion: 1.12.13</li>
</ul>
<h3>MiRo Workstation Setup</h3>
<p>Download the <a href="http://labs.consequentialrobotics.com/miro/mdk/">Miro Developer kit</a>.</p>
<p>Follow the instructions from Consequential Robotics <a href="https://consequential.bitbucket.io/Developer_Preparation_Prepare_workstation.html">Miro: Prepare Workstation</a> to set up your workstation to work with mthe robot. Strictly follow the instructions in the Install <b>mdk</b> section as the following steps will rely on this. Not necessary to make static IP for your workstation (laptop) while setting up connection with MiRo. For a clear tutorial step-by-step you should visit <a href="https://github.com/EmaroLab/MIRO.git">Emarolab Miro Repository</a>.</p>
<h3>The wearable device</h3>
<p>In order to interact with MiRo through gestures, a smartwatch with a 9-axis IMU sensor has been used. <a href="https://www.lg.com/wearable-technology/lg-G-Watch-R-W110">LG G WATCH R</a> Follow the instructions reported in <a href="https://github.com/EmaroLab/imu_stream">imu_stream</a> to download the app for both the smartphone and the smartwatch.</p>
<h3>Smartwatch and Smartphone Setup</h3>
<p>In order to publish imu sensor data from your smartwatch to ROS nodes you must have a smartwatch paired with a smartphone. The smartphone acts as the bridge between the smartwatch and the ros master running on your computer.</p>
<h3>MQTT ROS Bridge</h3>
<p>In order to succesfully subscribe to MQTT topics and publish contents of MQTT messages to ROS follow the instruction in <a href="https://github.com/EmaroLab/mqtt_ros_bridge/tree/feature/multiple_smartwatches">mqtt_ros_bridge</a>. To work with the current project some parameter must be modified in the imu_bridge.launch The parameter device_name must be changed with the name of your personal smartwatch.</p>
<h3>ROS Based Speech Interface</h3>
<p>In order to vocally interact with the robot we use a repository that contains an example for using a web interface to speak with the robot. It is based on Google Speech Demo for performing speech-to-text. We disabled the text-to-speech functionality.</p>
<p>For this project we used the mic in <a href="https://www.logitech.com/it-it/product/wireless-headset-h600">LOGITECH Wireless Headset H600</a>, but any microphone connected to your laptop should work pretty fine.</p>
<p>Create a catkin workspace and clone all the packages in the src folder</p>
<p>``` $ git clone <a href="https://github.com/EmaroLab/ros_verbal_interaction_node.git">https://github.com/EmaroLab/ros_verbal_interaction_node.git</a></p>
<p>```</p>
<p>For further information follow the instruction contained in <a href="https://github.com/EmaroLab/ros_verbal_interaction_node">ros_verbal_interaction_node</a> repository.</p>
<h3>OpenCV apps</h3>
<p>The images streaming from Miro's camera are processed using the package <a href="http://wiki.ros.org/opencv_apps">opencv_apps</a>. The camera frames are subject to color segmentation and hugh circles detection. To install it: ``` $ git clone sudo apt install ros-kinetic-opencv-apps</p>
<p>```</p>
<h3>MiRo Training</h3>
<p>Create a catkin workspace and clone all the packages in the src folder</p>
<p>``` $ git clone <a href="https://github.com/EmaroLab/MiRo-training.git">https://github.com/EmaroLab/MiRo-training.git</a> $ catkin_make $ source devel/setup.bash ```</p>
<h2>Run the Project</h2>
<p>Open a new terminal and launch</p>
<p>``` $ roscore ``` mosquitto must be running on your PC for the birdge to work.</p>
<p>In a new terminal ``` $ mosquitto ``` Make sure that the IP in the IMU_stream app on the smartphone is the same shown by doing</p>
<p>``` $ ifconfig ```</p>
<p>Open the IMU_stream app on the smartwatch To test if the connection between smartwatch and ROS is working, start to transmitt the data from IMU_stream app on the smartwatch and check in a new terminal ``` $ rostopic echo  ``` It should see the Imu data published by the smartwatch.</p>
<p>Connect the Miro robot to the ROS Master</p>
<p>``` $ ssh root&lt;MIRO-IP&gt; $ sudo nano ./profile ``` Insert your IP after ROS_MASTER_IP</p>
<p>For more detailed instructions see <a href="https://consequential.bitbucket.io/Developer_Preparation_Commission_MIRO.html">MIRO: Commission MIRO</a></p>
<p>Open in a new terminal your catkin_ws The following command will start the project</p>
<p>``` $ roslaunch command_handler command_handler.launch</p>
<p>```</p>
<p>Parameters that is possible to change directly from the launch file:</p>
<ul>
<li>Robot &ndash;&gt; sim01 or rob01 (Default value) To switch from real robot to simulation ( You should launch Gazebo with miro sim as explained in <a href="https://consequential.bitbucket.io/Developer_Preparation_Commission_MIRO.html">MIRO: Consequential Robotics</a></li>
<li>Node rate &ndash;&gt; 200 Hz (Default value)</li>
<li>Color of the ball to detect &ndash;&gt; h_limit_min = 0; h_limit_max = 10; s_limit_min = 255; s_limit_max = 150; v_limit_min = 255; v_limit_max = 50; Change the min and maximum HSV values to detect different colors. To discover the HSV values of your favorite color, check <a href="https://alloyui.com/examples/color-picker/hsv">this</a>.</li>
</ul>
<h2>Results</h2>
<p>Click the picture below for demostration video.</p>
<p><a href="https://www.youtube.com/watch?v=DoKFgs3enpU&amp;feature=youtu.be">![MiRo-Training - SoRo](https://img.youtube.com/vi/DoKFgs3enpU/0.jpg)</a>.</p>
<p>Each partecipant was asked to fill the <a href="http://bit.ly/MiroTrainingSurvey">questionaire</a> in order to evaluate the interaction with the robot. </p>
<h2>Recommendations</h2>
<h2>Acknowledgments</h2>
<ul>
<li><a href="https://github.com/EmaroLab/mqtt_ros_bridge">mqtt_ros_bridge</a></li>
<li><a href="https://github.com/EmaroLab/imu_stream">imu_stream</a></li>
</ul>
<h3>Team</h3>
<ul>
<li>Roberta Delrio *roberta<a href="#" onclick="location.href='mai'+'lto:'+'.de'+'lr'+'io@'+'st'+'udi'+'o.'+'uni'+'bo'+'.it'; return false;">.delr<span style="display: none;">.nosp@m.</span>io@s<span style="display: none;">.nosp@m.</span>tudio<span style="display: none;">.nosp@m.</span>.uni<span style="display: none;">.nosp@m.</span>bo.it</a>*</li>
<li>Valentina Pericu *valentina<a href="#" onclick="location.href='mai'+'lto:'+'.pe'+'ri'+'cu.'+'@g'+'mai'+'l.'+'com'; return false;">.peri<span style="display: none;">.nosp@m.</span>cu.@<span style="display: none;">.nosp@m.</span>gmail<span style="display: none;">.nosp@m.</span>.com</a>* </li>
</ul>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.6
</small></address>
</body>
</html>
